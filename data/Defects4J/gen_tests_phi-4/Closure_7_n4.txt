public void testBadTypeInference() {
    Object test = new Object() {
        public boolean isMatched(Object object) {
            if (goog.isDef(object)) {
                if (goog.isFunction(object)) {
                    // function case
                } else if (goog.isBoolean(object)) {
                    // boolean case
                } else if (goog.isString(object)) {
                    // string case
                } else if (goog.isArray(object)) {
                    int i = object.length; // This should cause a problem
                }
            }
            return false;
        }
    };

    // Invoke to trigger the type inference issue
    test.isMatched(new int[]{1, 2, 3});
}

Note: In this test case, the Java equivalent does not directly reflect the JavaScript execution environment. The key point is to trigger the type inference issue by accessing `object.length` on a Java array, which doesn't have a `length` property. This would not compile in Java as-is, but the intention is to simulate the JavaScript behavior where `object.length` is expected to be valid for arrays.